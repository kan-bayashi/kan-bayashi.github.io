<!doctype html>
<html lang="en-us">
  <head>
    <title> // Tomoki Hayashi</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.79.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Tomoki Hayashi" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://kan-bayashi.github.io/css/main.min.f4a4366db10002f954ba7c99eb4bd8ff403cc3f02a5115a04738873607cfc0d4.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="About Me  Name:  Tomoki Hayashi (Ph. D)   Affiliation:  COO @ Human Dataware Lab. Co., Ltd., Japan Postdoctroal researcher @ Nagoya University, Japan Researcher @ TARVO Inc., Japan   Research Interests:  Speech processing  Speech synthesis Speech recognition Voice conversion   Environmental sound processing  Sound event detection Anomalous sound detection   Time series processing  Demand forcast Anomaly detection      Bio Short Bio Tomoki Hayashi received the B."/>

    <meta property="og:title" content="" />
<meta property="og:description" content="About Me  Name:  Tomoki Hayashi (Ph. D)   Affiliation:  COO @ Human Dataware Lab. Co., Ltd., Japan Postdoctroal researcher @ Nagoya University, Japan Researcher @ TARVO Inc., Japan   Research Interests:  Speech processing  Speech synthesis Speech recognition Voice conversion   Environmental sound processing  Sound event detection Anomalous sound detection   Time series processing  Demand forcast Anomaly detection      Bio Short Bio Tomoki Hayashi received the B." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://kan-bayashi.github.io/" />
<meta property="article:published_time" content="2021-09-04T00:00:00+09:00" />
<meta property="article:modified_time" content="2021-09-04T00:00:00+09:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://kan-bayashi.github.io/"><img class="app-header-avatar" src="/avatar.png" alt="Tomoki Hayashi" /></a>
      <h1>Tomoki Hayashi</h1>
      <nav class="app-header-menu">
          <a class="app-header-menu-item" href="/#about-me">About</a>
             / 
          
          <a class="app-header-menu-item" href="/#bio">Bio</a>
             / 
          
          <a class="app-header-menu-item" href="/#memberships">Memberships</a>
             / 
          
          <a class="app-header-menu-item" href="/#publications">Publications</a>
             / 
          
          <a class="app-header-menu-item" href="/#softwares">Softwares</a>
      </nav>
      <p>      Speech processing, Environmental sound processing, Deep learning, Open-source software development.
  </p>
      <div class="app-header-social">
        
          <a target="_blank" href="mailto:hayashi.tomoki@g.sp.m.is.nagoya-u.ac.jp" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-mail">
  <title>mail</title>
  <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline>
</svg></a>
        
          <a target="_blank" href="https://github.com/kan-bayashi" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title"></h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Sep 4, 2021
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          14 min read
        </div>
      </div>
    </header>
    <div class="post-content">
      <h2 id="about-me">About Me</h2>
<ul>
<li>Name:
<ul>
<li>Tomoki Hayashi (Ph. D)</li>
</ul>
</li>
<li>Affiliation:
<ul>
<li>COO @ Human Dataware Lab. Co., Ltd., Japan</li>
<li>Postdoctroal researcher @ Nagoya University, Japan</li>
<li>Researcher @ TARVO Inc., Japan</li>
</ul>
</li>
<li>Research Interests:
<ul>
<li>Speech processing
<ul>
<li>Speech synthesis</li>
<li>Speech recognition</li>
<li>Voice conversion</li>
</ul>
</li>
<li>Environmental sound processing
<ul>
<li>Sound event detection</li>
<li>Anomalous sound detection</li>
</ul>
</li>
<li>Time series processing
<ul>
<li>Demand forcast</li>
<li>Anomaly detection</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="bio">Bio</h2>
<h4 id="short-bio">Short Bio</h4>
<p>Tomoki Hayashi received the B.E. degree in engineering and the M.E. and Ph.D. degrees in information science from Nagoya University, Aichi, Japan, in 2014, 2016, and 2019, respectively. His research interests include statistical speech and audio signal processing. He is currently working as a postdoctoral researcher at Nagoya University and the chief operating officer of Human Dataware Lab. Co., Ltd. He is a main developer of the end-to-end speech processing toolkit ESPnet. He received the IEEE SPS Japan 2020 Young Author Best Paper Award and the Itakura Award from the Acoustical Society of Japan.</p>
<h4 id="education">Education</h4>
<ul>
<li>Apr. 2010 - Mar. 2014
<ul>
<li>School of Engineering, Nagoya University, Japan
<ul>
<li>B.E. degree in Engineering, 1999</li>
<li>Superviser: <a href="https://scholar.google.com/citations?user=O4epWMcAAAAJ">Kazuya Takeda</a>, <a href="https://sites.google.com/site/norihidekitaokashome/home">Norihide Kitaoka</a></li>
</ul>
</li>
</ul>
</li>
<li>Apr. 2014 - Mar. 2019
<ul>
<li>Graduate School of Information Science, Nagoya University, Japan
<ul>
<li>Master degree in Information Science, 2016</li>
<li>Doctor degree in Information Science, 2019</li>
<li>Superviser: <a href="https://scholar.google.com/citations?user=O4epWMcAAAAJ">Kazuya Takeda</a>, <a href="https://sites.google.com/site/tomokitoda/">Tomoki Toda</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="work--research-experience">Work / Research experience</h4>
<ul>
<li>Aug. 2014 - Sep. 2014
<ul>
<li>NTT Communication Scienece Laborotories (Keihanna, Japan)
<ul>
<li>Research internship</li>
<li>Superviser: <a href="http://www.kecl.ntt.co.jp/icl/signal/araki/index_e.htm">Shoko Araki</a></li>
</ul>
</li>
</ul>
</li>
<li>Aug. 2016 - Nov. 2016
<ul>
<li>Mitsubishi Electric Research Laborotories (Boston, USA)
<ul>
<li>Research internship</li>
<li>Superviser: <a href="https://sites.google.com/view/shinjiwatanabe">Shinji Watanabe</a></li>
</ul>
</li>
</ul>
</li>
<li>Oct. 2017 - Dec. 2017
<ul>
<li>NEC Corporation (Kanagawa, Japan)
<ul>
<li>Research internship</li>
<li>Superviser: <a href="https://scholar.google.co.jp/citations?user=o2lMlxMAAAAJ">Tatsuya Komatsu</a></li>
</ul>
</li>
</ul>
</li>
<li>Nov. 2015 - Now
<ul>
<li>Human Dataware Lab. Co., Ltd. (Nagoya, Japan)
<ul>
<li>Chief operating officer</li>
</ul>
</li>
</ul>
</li>
<li>Apr. 2019 - Now
<ul>
<li>Nagoya Univerisy (Nagoya, Japan)
<ul>
<li>Postdoctoral researcher</li>
</ul>
</li>
</ul>
</li>
<li>Feb. 2020 - Now
<ul>
<li>TARVO Inc. (Nagoya, Japan)
<ul>
<li>Researcher</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="memberships">Memberships</h2>
<ul>
<li>The Institute of Electrical and Electronics Engineers, Inc. (IEEE), Member</li>
<li>International Speech Communication Association (ISCA), Member</li>
<li>The Acoustical Society of Japan (ASJ), Member</li>
</ul>
<h2 id="publications">Publications</h2>
<ul>
<li><a href="https://scholar.google.com/citations?user=75YkIWoAAAAJ">Google Scholar</a></li>
</ul>
<h3 id="award">Award</h3>
<ol>
<li>日本音響学会 独創研究奨励賞 板倉記念, Mar. 2021.</li>
<li>IEEE Signal Processing Society Japan Young Author Best Paper Award, Dec, 2020.</li>
<li>DCASE2020 Challenge Task 2 Judge&rsquo;s award, Nov. 2020.</li>
<li>日本音響学会東海支部 優秀発表賞 Jul. 2015.</li>
<li>日本音響学会秋季研究発表会 学生優秀発表賞, Sep. 2014.</li>
</ol>
<h3 id="tutorials--invited-talks">Tutorials / Invited talks</h3>
<ol>
<li><u><strong>林 知樹</strong></u>, 山本 龍一, 井上 勝喜, 吉村 建慶, 武田 一哉, 戸田 智基, 渡部 晋治, &ldquo;End-to-end音声合成の研究を加速させるオープンソースツールキットESPnet-TTS,&rdquo; 日本音響学会春季研究発表会 スペシャルセッション「end-to-end音声合成とその周辺」, Mar. 2020.（招待講演）</li>
<li>T. Hori, <u><strong>T. Hayashi</strong></u>, S. Karita, and S. Watanabe, &ldquo;Advanced Methods for Neural End-to-End Speech Processing - Unification, Integration, and Implementation,&rdquo; Interspeech Tutorial, Sep. 2019.</li>
<li>T. Toda, K. Kobayashi, and <u><strong>T. Hayashi</strong></u>, &ldquo;Statistical voice conversion with direct waveform modeling,&rdquo; Interspeech Tutorial, Sep. 2019.</li>
</ol>
<h3 id="review-papers">Review papers</h3>
<ol>
<li><u><strong>林 知樹</strong></u>, &ldquo;End-to-End音声処理の概要とESPnet2を用いたその実践,&rdquo; 日本音響学会誌，Vol. 76, No. 12, pp. 720-729, Dec. 2020．</li>
<li><u><strong>林 知樹</strong></u>, 戸田 智基, &ldquo;統計的手法による音響イベント検出,&rdquo; 日本音響学会誌，Vol. 75, No. 9, pp. 532-537, Sep. 2019．</li>
<li>K. Miyazaki, T. Toda, <u><strong>T. Hayashi</strong></u>, K. Takeda, &ldquo;Environmental sound processing and its applications,&rdquo; IEEJ Transactions on Electronics, Information and Systems，Vol. 14, No. 3, pp. 340-351, Mar. 2019.</li>
</ol>
<h3 id="journal-papers">Journal papers</h3>
<ol>
<li>Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, P.L. Tobing, K. Kobayashi, T. Toda, &ldquo;Quasi-periodic WaveNet: an autoregressive raw waveform generative model with pitch-dependent dilated convolution neural network,&rdquo; IEEE/ACM Transactions on Audio, Speech and Language Processing, Vol. 29, pp. 1134-1148, 2021.</li>
<li>Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, T. Okamoto, H. Kawai, T. Toda, &ldquo;Quasi-periodic parallel WaveGAN: a non-autoregressive raw waveform generative model with pitch-dependent dilated convolution neural network,&rdquo; IEEE/ACM Transactions on Audio, Speech and Language Processing, Vol. 29, pp. 792-806, 2021.</li>
<li>W.-C. Huang, <u><strong>T. Hayashi</strong></u>, Y.-C. Wu, H. Kameoka, T. Toda, &ldquo;Pretraining techniques for sequence-to-sequence voice conversion,&rdquo; IEEE/ACM Transactions on Audio, Speech and Language Processing, pp. 745-755, 2021.</li>
<li>P.L. Tobing, Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;An evaluation of voice conversion with neural network spectral mapping models and WaveNet vocoder,&rdquo; APSIPA Transactions on Signal and Information Processing, Vol. 9, e26, pp. 1-14, Nov. 2020.</li>
<li>Y.-C. Wu, P.L. Tobing, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;Non-parallel voice conversion system with WaveNet vocoder and collapsed speech suppression,&rdquo; IEEE Access, Vol. 8, No. 1, pp. 62094-62106, Apr. 2020.</li>
<li>P.L. Tobing, Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;Voice conversion with CycleRNN-based spectral mapping and finly tuned WaveNet vocoder,&rdquo; IEEE Access, Vol. 7, No. 1, pp. 171114-171125, Dec. 2019.</li>
<li>A. Tamamori, <u><strong>T. Hayashi</strong></u>, T. Toda, K. Takeda, &ldquo;Daily activity recognition based on recurrent neural network using multi-modal signals,&rdquo; APSIPA Transactions on Signal and Information Processing, Vol. 7, e21, pp..1-11, Dec. 2018.</li>
<li><u><strong>T. Hayashi</strong></u>, M. Nishida, N. Kitaoka, T. Toda, K. Takeda, &ldquo;Daily activity recognition with large-scaled real-life recording datasets based on deep neural network using multi-modal signals,&rdquo; IEICE Transactions on Fundamentals, Vol. E101-A, No. 1, pp. 199-210, Jan. 2018.</li>
<li>S. Watanabe, T. Hori, S. Kim, J. R. Hershey, <u><strong>T. Hayashi</strong></u>, &ldquo;Hybrid CTC/Attention architecture for end-to-end speech recognition,&rdquo; in IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240-1253, Dec. 2017.</li>
<li><u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, T. Hori, J. Le Roux, K. Takeda, &ldquo;Duration-controlled LSTM for polyphonic sound event detection,&rdquo; IEEE/ACM Transactions on Audio, Speech, and Language Processing, Vol. 25, No. 11, pp. 2059-2070, Nov. 2017. <strong>【IEEE Signal Processing Society Japan 2020 Young Author Best Paper Award】</strong></li>
</ol>
<h3 id="international-conference">International conference</h3>
<ol>
<li>T. Komatsu, S. Watanabe, K. Miyazaki, <u><strong>T. Hayashi</strong></u>, &ldquo;Acoustic Event Detection with Classifier Chains,&rdquo; Proc. INTERSPEECH, 2021 (Accepted).</li>
<li>I. Kuroyanagi, <u><strong>T. Hayashi</strong></u>, K. Takeda, T. Toda, &ldquo;Anomalous sound detection using a binary classification model and class centroids,&rdquo; Proc. EUSIPCO, 2021. (Accepted)</li>
<li><u><strong>T. Hayashi</strong></u>, T. Yoshimura, M. Inuzuka, I. Kuroyanagi, O. Segawa, &ldquo;Spontaneous speech summarization: Transformers all the way through,&rdquo; Proc. EUSIPCO, 2021. (Accepted)</li>
<li><u><strong>T. Hayashi</strong></u>, W.-C. Huang, K. Kobayashi, T. Toda, &ldquo;Non-autoregressive sequence-to-sequence voice conversion,&rdquo; Proc. ICASSP, 2021. (Accepted)</li>
<li>P. Guo, F. Boyer, X. Chang, <u><strong>T. Hayashi</strong></u>, Y. Higuchi, H. Inaguma, N. Kamo, C. Li, D. G. Romero, J. Shi, J. Shi, S. Watanabe, K. Wei, W. Zhang, Y. Zhang, &ldquo;Recent Developments on ESPnet Toolkit Boosted by Conformer,&rdquo; Proc. ICASSP, 2021. (Accepted)</li>
<li>K. Kobayashi, W.-C. Huang, Y.-C. Wu, S. P.L. Tobing, <u><strong>T. Hayashi</strong></u>, T. Toda, &ldquo;Crank: an open-source software for nonparallel voice conversion based on vector-quantized variational autoencoder,&rdquo; Proc. ICASSP, 2021. (Accepted)</li>
<li>W.-C. Huang, Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, T. Toda, &ldquo;Any-to-one sequence-to-sequence voice conversion using self-supervised discrete speech representations,&rdquo; Proc. ICASSP, 2021. (Accepted)</li>
<li>C. Li, J. Shi, W. Zhang, A. S. Subramanian, X. Chang, N. Kamo, M. Hira, <u><strong>T. Hayashi</strong></u>, C. Boeddeker, Z. Chen, S. Watanabe, &ldquo;ESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,&rdquo; Proc. IEEE SLT, pp. 785-792, Dec. 2020.</li>
<li>K. Miyazaki, T. Komatsu, <u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, K. Takeda, &ldquo;Conformer-based sound event detection with semi-supervised learning and data augmentation,&rdquo; Proc. DCASE 2020 Workshop, pp. 100-104, Nov. 2020.</li>
<li>W.-C. Huang, <u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, &ldquo;The sequence-to-sequence baseline for the Voice Conversion Challenge 2020: cascading ASR and TTS,&rdquo; Proc. Joint workshop for the Blizzard Challenge and Voice Conversion Challenge 2020, pp. 160-164, Oct. 2020.</li>
<li>W.-C. Huang, <u><strong>T. Hayashi</strong></u>, Y.-C. Wu, H. Kameoka, T. Toda, &ldquo;Voice transformer network: sequence-to-sequence voice conversion using transformer with text-to-speech pretraining,&rdquo; Proc. INTERSPEECH, pp. 4675-4680, Oct. 2020.</li>
<li>Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, T. Okamoto, H. Kawai, T. Toda, &ldquo;Quasi-periodic parallel WaveGAN vocoder: a non-autoregressive pitch-dependent dilated convolution model for parametric speech generation,&rdquo; Proc. INTERSPEECH, pp. 3535-3539, Oct. 2020.</li>
<li>S. Hikosaka, S. Seki, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, K. Takeda, H. Banno, T. Toda, &ldquo;Intelligibility enhancement based on speech waveform modification using hearing impairment simulator,&rdquo; Proc. INTERSPEECH, pp. 4059-4063, Oct. 2020.</li>
<li>P.L. Tobing, <u><strong>T. Hayashi</strong></u>, Y.-C. Wu, K. Kobayashi, T. Toda, &ldquo;Cyclic spectral modeling for unsupervised unit discovery into voice conversion with excitation and waveform modeling,&rdquo; Proc. INTERSPEECH, pp. 3540-3544, Oct. 2020.</li>
<li>H. Inaguma, S. Kiyono, K. Duh, S. Karita, N. E. Yalta Soplin, <u><strong>T. Hayashi</strong></u>, S. Watanabe, &ldquo;ESPnet-ST: All-in-One Speech Translation Toolkit,&rdquo; Proc. the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 302-311, Full virtual, Jul. 2020.</li>
<li>T. Yoshimura, <u><strong>T. Hayashi</strong></u>, K. Takeda, S. Watanabe, &ldquo;End-to-end automatic speech recognition integrated with ctc-based voice activity detection,&rdquo; Proc. IEEE ICASSP, pp. 6999-7003, Full virtual, May 2020.</li>
<li>K. Inoue, S. Hara, M. Abe, <u><strong>T. Hayashi</strong></u>, R. Yamamoto, S. Watanabe, &ldquo;Semi-Supervised Speaker Adaptation for End-to-End Speech Synthesis with Pretrained Models,&rdquo; Proc. IEEE ICASSP, pp. 7634-7638, Full virtual, May 2020.</li>
<li>K. Miyazaki, T. Komatsu, <u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, K. Takeda, &ldquo;Weakly-supervised sound event detection with self-attention,&rdquo; Proc. IEEE ICASSP, pp. 66-70, Full virtual, May 2020.</li>
<li>P.L. Tobing, Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;Efficient shallow WaveNet vocoder using multiple samples output based on Laplacian distribution and linear prediction,&rdquo; Proc. IEEE ICASSP, pp. 7204-7208, Full virtual, May 2020.</li>
<li><u><strong>T. Hayashi</strong></u>, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang, X. Tan, &ldquo;ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit,&rdquo; Proc. IEEE ICASSP, pp. 7654-7658, Full virtual, May 2020.</li>
<li>S. Karita, N. Chen, <u><strong>T. Hayashi</strong></u>, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Yalta Soplin, R. Yamamoto, X. Wang, S. Watanabe, T. Yoshimura, and W. Zhang, &ldquo;A comparative study on Transformer vs RNN in speech applications,&rdquo; Proc. IEEE ASRU, pp. 449-456, Sentosa, Singapore, Dec. 2019.</li>
<li>P.L. Tobing, <u><strong>T. Hayashi</strong></u>, T. Toda, &ldquo;Investigation of shallow WaveNet vocoder with Laplacian distribution output,&rdquo; Proc. IEEE ASRU, pp. 176-183, Sentosa, Singapore, Dec. 2019.</li>
<li>O. Segawa, T. Hayashi, K. Takeda, &ldquo;Attention-Based Speech Recognition Using Gaze Information,&rdquo; Proc. IEEE ASRU, pp. 465-470, Sentosa, Singapore, Dec. 2019.</li>
<li>Y.-C. Wu, P.L. Tobing, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;Statistical voice conversion with quasi-periodic WaveNet vocoder,&rdquo; Proc. 10th ISCA Speech Synthesis Workshop (SSW10), pp. 63-68, Vienna, Austria, Sep. 2019.</li>
<li>Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, P.L. Tobing, K. Kobayashi, T. Toda, &ldquo;Quasi-periodic WaveNet vocoder: a pitch dependent dilated convolution model for parametric speech generation,&rdquo; Proc. INTERSPEECH, pp. 196-200, Graz, Austria, Sep. 2019.</li>
<li>P.L. Tobing, Y.-C. Wu, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;Non-parallel voice conversion with cyclic variational autoencoder,&rdquo; Proc. INTERSPEECH, pp. 674-678, Graz, Austria, Sep. 2019.</li>
<li>W.-C. Huang, Y.-C. Wu, C.-C. Lo, P.L. Tobing, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, Y. Tsao, H.-M. Wang, &ldquo;Investigation of F0 conditioning and fully convolutional networks in variational autoencoder based voice conversion,&rdquo; Proc. INTERSPEECH, pp. 709-713, Graz, Austria, Sep. 2019.</li>
<li><u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, K. Takeda, S. Toshniwal, K. Livescu, &ldquo;Pre-trained text embeddings for enhanced text-to-speech synthesis,&rdquo; Proc. INTERSPEECH, pp. 4430-4434, Graz, Austria, Sep. 2019.</li>
<li>W.-C. Huang, Y.-C. Wu, H.-T. Hwang, P.L. Tobing, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, Y. Tsao, H.-M. Wang, &ldquo;Refined WaveNet vocoder for variational autoencoder based voice conversion,&rdquo; Proc. EUSIPCO, 5 pages, A Coruna, Spain, Sep. 2019.</li>
<li>T. Hori, R. Astudillo, <u><strong>T. Hayashi</strong></u>, Y. Zhang, S. Watanabe, and J. LeRoux, &ldquo;Cycle-consistency training for end-to-end speech recognition,&rdquo; Proc. IEEE ICASSP, pp. 6271-6275, Brighton, UK, May 2019.</li>
<li>T. Komatsu, <u><strong>T. Hayashi</strong></u>, R. Kondo, T. Toda, K. Takeda, &ldquo;Scene-dependent anomalous acoustic-event detection based on conditional WaveNet and i-Vector,&rdquo; Proc. IEEE ICASSP, pp. 870-874, Brighton, UK, May 2019.</li>
<li>P.L. Tobing, Y. Wu, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;Voice conversion with cyclic recurrent neural network and fine-tuned WaveNet vocoder,&rdquo; Proc. IEEE ICASSP, pp. 6815-6819, Brighton, UK, May 2019.</li>
<li><u><strong>T. Hayashi</strong></u>, S. Watanabe, Y. Zhang, T. Toda, T. Hori, R. Astudillo, K. Takeda, &ldquo;Back-translation-style data augmentation for end-to-end ASR,&rdquo; Proc. IEEE SLT, pp. 426-433, Dec. 2018.</li>
<li>P. L. Tobing, <u><strong>T. Hayashi</strong></u>, Y. Wu, K. Kobayashi, T. Toda, &ldquo;An evaluation of deep spectral mappings and WaveNet vocoder for voice conversion,&rdquo; Proc. IEEE SLT, pp. 297-303, Dec. 2018.</li>
<li>K. Miyazaki, <u><strong>T. Hayashi</strong></u>, T. Toda, K. Takeda, &ldquo;Connectionist temporal classification-based sound event encoder for converting sound events into onomatopoeia representations,&rdquo; Proc. EUSIPCO, pp. 857-861, Sep. 2018.</li>
<li><u><strong>T. Hayashi</strong></u>, T. Komatsu, R. Kondo, T. Toda, K. Takeda, &ldquo;Anomalous sound event detection based on WaveNet,&rdquo; Proc. EUSIPCO, pp. 2508-2512, Sep. 2018.</li>
<li><u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, K. Takeda, &ldquo;Multi-head decoder for end-to-end speech recognition,&rdquo; Proc. INTERSPEECH, pp. 801-805, Sep. 2018.</li>
<li>Y. Wu, K. Kobayashi, <u><strong>T. Hayashi</strong></u>, P. L. Tobing, T. Toda, &ldquo;Collapsed segment detection and reduction for WaveNet vocoder,&rdquo; Proc. INTERSPEECH, pp. 1998-1992, Sep. 2018.</li>
<li>Y. Wu, P. L. Tobing, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;The NU non-parallel voice conversion system for the voice conversion challenge 2018,&rdquo; Proc. Odyssey 2018, pp. 211-218, June 2018.</li>
<li>P. L. Tobing, Y. Wu, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;NU voice conversion system for the voice conversion challenge 2018,&rdquo; Proc. Odyssey 2018, pp. 219-226, June 2018.</li>
<li><u><strong>T. Hayashi</strong></u>, A. Tamamori, K. Kobayashi, K. Takeda, T. Toda, &ldquo;An investigation of multi-speaker training for WaveNet vocoder,&rdquo; Proc. ASRU, pp. 712-718, Dec. 2017.</li>
<li>A. Tamamori, <u><strong>T. Hayashi</strong></u>, T. Toda, K. Takeda, &ldquo;Investigation of effectiveness on recurrent neural network for daily activity recognition using multi-modal signals,&rdquo; Proc. APSIPA, 7 pages, Kuala Lumpur, Malaysia, Dec. 2017.</li>
<li>A. Tamamori, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, K. Takeda, T. Toda, &ldquo;Speaker-dependent WaveNet vocoder,&rdquo; Proc. INTERSPEECH, pp. 1118-1122, Aug. 2017.</li>
<li>K. Kobayashi, <u><strong>T. Hayashi</strong></u>, A. Tamamori, T. Toda, &ldquo;Statistical voice conversion with WaveNet-based waveform generation,&rdquo; Proc. INTERSPEECH, pp. 1138-1142, Aug. 2017.</li>
<li><u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, T. Hori, J. Le Roux, K. Takeda, &ldquo;BLSTM-HMM hybrid system combined with sound activity detection network for polyphonic sound event detection,&rdquo; Proc. ICASSP, pp. 766-770, Mar. 2017.</li>
<li>A. Tamamori, <u><strong>T. Hayashi</strong></u>, T. Toda, K. Takeda, &ldquo;Investigation on recurrent neural network architectures for daily activity recognition,&rdquo; Proc. UV2016, Oct. 2016.</li>
<li><u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, T. Hori, J. Le Roux, K. Takeda, &ldquo;Bidirectional LSTM-HMM hybrid system for polyphonic sound event detection,&rdquo; Proc. DCASE2016 workshop, 5 pages, Sep. 2016.</li>
<li>S. Araki, <u><strong>T. Hayashi</strong></u>, M. Delcroix, M. Fujimoto, K. Takeda, T. Nakatani, &ldquo;Exploring multi-channel features for denoising-autoencoder-based speech enhancement,&rdquo; Proc. ICASSP, pp.116-120, Apr. 2015.</li>
<li><u><strong>T. Hayashi</strong></u>, S. Watanabe, T. Toda, T. Tori, J. LeRoux, K. Takeda, &ldquo;Convolutional bidirectional long short-term memory hidden Markov model hybrid system for polyphonic sound event detection,&rdquo; Proc. 5th Joint Meeting of the Acoustical Society of America and Acoustical Society of Japan, Dec. 2016.</li>
<li>H. Erdogan, <u><strong>T. Hayashi</strong></u>, J. R. Hershey, T. Hori, C. Hori, W. Hsu, S. Kim, J. LeRoux, Z. Meng, S. Watanabe, &ldquo;Multi-channel speech recognition: LSTMs all the way through,&rdquo; Proc. CHiME4 workshop, 2016.</li>
<li><u><strong>T. Hayashi</strong></u>, M. Nishida, N. Kitaoka, K. Takeda, &ldquo;Daily activity recognition based on DNN using environmental sound and acceleration signals,&rdquo; Proc. EUSIPCO, pp. 2351-2355, Sep. 2015.</li>
<li>N. Kitaoka, <u><strong>T. Hayashi</strong></u>, K. Takeda, &ldquo;Noisy speech recognition using blind spatial subtraction array technique and deep bottleneck features,&rdquo; Proc. APSIPA, 4 pages, Dec. 2014.</li>
<li><u><strong>T. Hayashi</strong></u>, N. Kitaoka, C. Miyajima, K. Takeda, &ldquo;Investigating the robustness of deep bottleneck features for recognizing speech of speakers of various ages&rdquo;, Proc. FORUM ACUSTICUM, 4 pages, Sep, 2014</li>
</ol>
<h3 id="domestic-conference">Domestic conference</h3>
<ol>
<li>畔栁 伊吹, <u><strong>林 知樹</strong></u>, 武田 一哉, 戸田 智基, &ldquo;特徴量空間のクラス重心を考慮した二値分類モデルによる異常音検知,&rdquo; 信学技報, Vol. 120, No. 397, EA2020-79, pp. 114-121, Mar. 2021.</li>
<li>宮崎 晃一, 小松 達也, <u><strong>林 知樹</strong></u>, 渡部 晋治, 戸田 智基, 武田 一哉, &ldquo;Self-attentionを用いた弱教師あり音響イベント検出,&rdquo; 音講論, 1-1-5, pp. 181-182, Mar. 2020.</li>
<li>彦坂 秀, 小林 和弘, <u><strong>林 知樹</strong></u>, 関 翔悟, 武田 一哉, 坂野 秀樹, 戸田 智基, &ldquo;模擬難聴処理を活用した補聴器フィルタ設計,&rdquo; 音講論, 1-6-6, pp. 567-568, Sep. 2019.</li>
<li>安原 和輝, <u><strong>林 知樹</strong></u>, 戸田 智基, &ldquo;End-to-End型テキスト音声合成におけるWaveNetボコーダの学習に関する調査,&rdquo; 音講論, 1-4-9, pp. 951-952, Sep. 2019.</li>
<li>彦坂 秀, 小林 和弘, <u><strong>林 知樹</strong></u>, 関 翔悟, 武田 一哉, 坂野 秀樹, 戸田 智基, &ldquo;模擬難聴処理を活用した音声波形加工に基づく明瞭度改善,&rdquo; 信学技報, Vol. 119, No. 188, SP2019-13, pp. 25-29, Aug. 2019.</li>
<li>安原 和輝, <u><strong>林 知樹</strong></u>, 戸田 智基, &ldquo;End-to-End型テキスト音声合成におけるWaveNetボコーダの学習についての調査,&rdquo; 信学技報, Vol. 119, No. 188, SP2019-14, pp. 31-36, Aug. 2019.</li>
<li><u><strong>林 知樹</strong></u>, 渡部 晋治, 戸田 智基, 武田 一哉, &ldquo;End-to-End音声認識ためのmulti-head decoderネットワーク,&rdquo; 音講論, pp. 925-926, Sep. 2018.</li>
<li>関 翔悟, <u><strong>林 知樹</strong></u>, 武田 一哉, 戸田 智基, &ldquo;WaveNetに基づく振幅スペクトログラムからの波形生成,&rdquo; 音講論, pp. 281-282, Sep. 2018.</li>
<li>宮崎 晃一, <u><strong>林 知樹</strong></u>, 戸田 智基, 武田 一哉, &ldquo;End-to-Endアプローチに基づく音イベントの擬音語表現への記号化,&rdquo; 信学技報, Vol. 118, No. 198, SP2018-30, pp. 37-42, Aug. 2018.</li>
<li>Y. Wu, P.L. Tobing, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;Development of NU non-parallel voice conversion system for Voice Conversion Challenge 2018,&rdquo; 音講論, pp. 217-218, Mar. 2018.</li>
<li>P. L. Tobing, Y. Wu, <u><strong>T. Hayashi</strong></u>, K. Kobayashi, T. Toda, &ldquo;Development of NU voice conversion system for Voice Conversion Challenge 2018,&rdquo; 音講論, pp. 215-216, Mar. 2018.</li>
<li><u><strong>林 知樹</strong></u>, 小林 和弘, 玉森 聡, 武田 一哉, 戸田 智基, &ldquo;WaveNetボコーダにおける学習データ量の影響に関する調査,&rdquo; 音講論, pp. 249-250, Mar. 2018.</li>
<li><u><strong>林 知樹</strong></u>, 小林 和弘, 玉森 聡, 武田 一哉, 戸田 智基, &ldquo;複数話者WaveNetボコーダに関する調査,&rdquo; 信学技報, Vol. 117, No. 393, SP2017-81, pp. 81-86, Jan. 2018.</li>
<li>小林 和弘, <u><strong>林 知樹</strong></u>, 玉森 聡, 戸田 智基, &ldquo;WaveNetボコーダを用いた統計的音声変換法.   信学技報,&rdquo; Vol. 117, No. 393, SP2017-82, pp. 87-92, Jan. 2018.</li>
<li>野田 聖太, <u><strong>林 知樹</strong></u>, 戸田 智基, 武田 一哉, &ldquo;DNN適応に基づく非可聴つぶやき認識用話者・環境依存音響モデルの構築,&rdquo; 信学技報, Vol. 117, No. 368, EA2017-56, pp. 7-10, Dec. 2017.</li>
<li>宮崎 晃一, <u><strong>林 知樹</strong></u>, 戸田 智基, 武田 一哉, &ldquo;CTCに基づく音響イベントから擬音語表現への変換,&rdquo; 音講論, pp. 19-20, Sep. 2017.</li>
<li><u><strong>林 知樹</strong></u>, 玉森 聡, 小林 和弘, 武田 一哉, 戸田 智基, &ldquo;WaveNetボコーダ学習における複数話者音声データの利用に関する検討,&rdquo; 音講論, pp. 285-286, Sep. 2017.</li>
<li>渡部 晋治，堀 貴明，<u><strong>林 知樹</strong></u>, キム スヨン,&ldquo;形態素解析も辞書も言語モデルも要らないend-to-end日本語音声認識,&rdquo; 音講論, pp. 89-90, Mar. 2017.</li>
<li>野田 聖太, <u><strong>林 知樹</strong></u>, 戸田 智基, 武田 一哉, &ldquo;非可聴つぶやき認識のための通常音声を活用したDNN音響モデル学習,&rdquo; 音講論, pp. 89-90, Mar. 2017.</li>
<li><u><strong>林 知樹</strong></u>，渡部 晋治，戸田 智基，堀 貴明，Jonathan Le Roux, 武田 一哉, &ldquo;イベント区間検出統合型BLSTM-HMMハイブリッドモデルによる多重音響イベント検出,&rdquo; 音講論, pp. 45-46, Mar. 2017.</li>
<li><u><strong>林 知樹</strong></u>, 渡部 晋治, 戸田 智基, 堀　貴明, Jonathan Le Roux, 武田 一哉, &ldquo;イベント継続長を明示的に制御したBLSTM-HSMMハイブリッドモデルによる多重音響イベント検出,&rdquo; 信学技報, Vol. 117, No. 138, EA2017-2, pp. 9-14, July 2017.</li>
<li>玉森 聡, <u><strong>林 知樹</strong></u>, 戸田 智基, 武田 一哉, &ldquo;音声生成過程を考慮したWaveNetに基づく音声波形合成法,&rdquo; 信学技報, Vol. 116, No. 477, SP2016-77, pp. 1-6, Mar. 2017.</li>
<li>玉森 聡, <u><strong>林 知樹</strong></u>, 戸田 智基, 武田 一哉, &ldquo;日常生活行動認識のためのrecurrent neural network構造の調査,&rdquo; 音講論, pp. 1-2, Sep. 2016.</li>
<li>玉森 聡，<u><strong>林 知樹</strong></u>，戸田 智基, 武田 一哉, &ldquo;Deep recurrent neural networkに基づく日常生活行動認識,&rdquo; 信学技報, Vol. 116, No. 189, SP2016-28, pp. 7-12, Aug. 2016.</li>
<li><u><strong>林 知樹</strong></u>，北岡 教英，戸田 智基, 武田 一哉, &ldquo;Deep neural networkに基づく日常生活行動認識における適応手法,&rdquo; 信学技報, Vol. 116, No. 189, SP2016-27, pp. 1-6, Aug. 2016.</li>
<li><u><strong>林 知樹</strong></u>，大谷 健登，武田 一哉，&ldquo;DNNによる不可逆圧縮音源の高音質化の検討,&rdquo; 日本音響学会秋季研究発表会, pp. 537-538, Sep. 2015</li>
<li>荒木章子, <u><strong>林知樹</strong></u> 他, &ldquo;マルチチャネル特徴を用いたdenoising autoencoder による音声強調,&rdquo; 音講論, pp. 685-686, Mar. 2015.</li>
<li><u><strong>林 知樹</strong></u>，西田 昌史，北岡 教英，武田 一哉，&ldquo;DNNによる環境音と加速度信号を用いた日常生活行動認識,&rdquo; 音講論, pp. 83-86，Mar. 2015.</li>
<li><u><strong>林 知樹</strong></u>，北岡 教英，武田 一哉, &ldquo;深層学習を用いた音声特徴量の年齢の変動に対する頑健性の調査,&rdquo; 音講論, pp.77-80, Sep, 2014.</li>
</ol>
<h3 id="others">Others</h3>
<ol>
<li>I. Kuroyanagi, <u><strong>T. Hayashi</strong></u>, Y. Adachi, T. Yoshimura, K. Takeda, T. Toda, &ldquo;ANOMALOUS SOUND DETECTION WITH ENSEMBLE OF AUTOENCODER AND BINARY CLASSIFICATION APPROACHES,&rdquo; DCASE2021 Challenge technical report, Jul. 2021.</li>
<li>C. Narisetty, T. Hayashi, R. Ishizaki, S. Watanabe, K. Takeda, &ldquo;Leveraging State-of-the-art ASR Techniques to Audio Captioning,&rdquo;  DCASE2021 Challenge technical report, Jul. 2021.</li>
<li><u><strong>T. Hayashi</strong></u>，T. Yoshimura, Y. Adachi, &ldquo;CONFORMER-BASED ID-AWARE AUTOENCODER FOR UNSUPERVISED ANOMALOUS SOUND DETECTION,&rdquo; DCASE2020 Challenge technical report, Jul. 2020.</li>
</ol>
<h2 id="softwares">Softwares</h2>
<p>Please do not ask about me about the codebase through e-mail. Please use github issue.</p>
<ul>
<li><a href="https://github.com/espnet/espnet">ESPnet</a>: End-to-end speech processing toolkit.</li>
<li><a href="https://github.com/kan-bayashi/ParallelWaveGAN">ParallelWaveGAN</a>: Unofficial ParallelWaveGAN (+ various GAN vocoders) implementation.</li>
<li><a href="https://github.com/kan-bayashi/PytorchWaveNetVocoder">PytorchWaveNetVocoder</a>: WaveNet vocoder with noise shaping implementation.</li>
</ul>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
